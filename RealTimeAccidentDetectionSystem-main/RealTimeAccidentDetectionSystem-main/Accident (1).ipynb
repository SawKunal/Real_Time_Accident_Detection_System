{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOL9A1JlT2Hp",
        "outputId": "b8aad3ac-d066-48f2-8845-7dcb7973c01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "TensorFlow 2.19.0\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 0 — GPU check & mount Drive\n",
        "import sys, os\n",
        "import tensorflow as tf\n",
        "print(\"Python\", sys.version)\n",
        "print(\"TensorFlow\", tf.__version__)\n",
        "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Mount Drive (will prompt for OAuth)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — Config & install deps\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/AccidentProject\"\n",
        "DATA_RAW = os.path.join(PROJECT_DIR, \"data\", \"raw\")\n",
        "FEATURE_DIR = os.path.join(PROJECT_DIR, \"data\", \"features\")\n",
        "CHECKPOINT_DIR = os.path.join(PROJECT_DIR, \"checkpoints\")\n",
        "LOG_DIR = os.path.join(PROJECT_DIR, \"logs\")\n",
        "EXPORT_DIR = os.path.join(PROJECT_DIR, \"exports\", \"models\")\n",
        "os.makedirs(DATA_RAW, exist_ok=True)\n",
        "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "# Install packages if not present\n",
        "!pip install -q tensorflow-addons opencv-python-headless tqdm scikit-learn\n",
        "\n",
        "# Enable mixed precision for speed (optional but recommended)\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"Mixed precision policy:\", mixed_precision.global_policy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBHCB7wOT8dD",
        "outputId": "caa745c1-56a1-4473-9dd6-23566dfb1acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-addons\u001b[0m\u001b[31m\n",
            "\u001b[0mMixed precision policy: <DTypePolicy \"mixed_float16\">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 — imports and small helpers\n",
        "import numpy as np, cv2, glob, json, math, random, time\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "print(\"Imports ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYlYj3qET90y",
        "outputId": "ae15d702-500b-4cd2-b800-73867da5d413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 — timestamp parsing and label loader (expects HH:MM:SS - HH:MM:SS per line)\n",
        "def to_seconds(hms):\n",
        "    hms = str(hms).strip()\n",
        "    parts = [int(p) for p in hms.split(\":\")]\n",
        "    if len(parts) == 3:\n",
        "        h,m,s = parts\n",
        "    elif len(parts) == 2:\n",
        "        h = 0; m,s = parts\n",
        "    else:\n",
        "        raise ValueError(\"Bad timestamp: \" + hms)\n",
        "    return h*3600 + m*60 + s\n",
        "\n",
        "def load_intervals(txt_path):\n",
        "    intervals = []\n",
        "    if not os.path.exists(txt_path):\n",
        "        return intervals\n",
        "    for line in open(txt_path, 'r'):\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        # handle \"HH:MM:SS - HH:MM:SS\" or \"HH:MM:SS to HH:MM:SS\"\n",
        "        if \"-\" in line:\n",
        "            a,b = line.split(\"-\")\n",
        "        elif \"to\" in line:\n",
        "            a,b = line.split(\"to\")\n",
        "        else:\n",
        "            continue\n",
        "        try:\n",
        "            intervals.append((to_seconds(a), to_seconds(b)))\n",
        "        except:\n",
        "            continue\n",
        "    return intervals\n",
        "\n",
        "def window_overlaps(start_t, end_t, intervals):\n",
        "    for a,b in intervals:\n",
        "        if start_t <= b and end_t >= a:\n",
        "            return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "ONr_9S88UAqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 — CCTV-safe augmentations without imgaug (use this instead)\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "def adjust_brightness(frame, factor):\n",
        "    # factor between 0.8 and 1.2\n",
        "    return np.clip(frame * factor, 0, 255).astype(np.uint8)\n",
        "\n",
        "def adjust_contrast(frame, factor):\n",
        "    # simple contrast adjustment\n",
        "    mean = frame.mean(axis=(0,1), keepdims=True)\n",
        "    return np.clip((frame - mean) * factor + mean, 0, 255).astype(np.uint8)\n",
        "\n",
        "def gaussian_noise(frame, std=10):\n",
        "    noise = np.random.normal(0, std, frame.shape)\n",
        "    out = frame + noise\n",
        "    return np.clip(out, 0, 255).astype(np.uint8)\n",
        "\n",
        "def jpeg_compress(frame, quality=60):\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
        "    _, enc = cv2.imencode('.jpg', frame[:,:,::-1], encode_param)\n",
        "    dec = cv2.imdecode(enc, cv2.IMREAD_COLOR)[:,:,::-1]\n",
        "    return dec\n",
        "\n",
        "def motion_blur(frame, ksize=3):\n",
        "    kernel = np.zeros((ksize, ksize))\n",
        "    kernel[int((ksize-1)/2), :] = 1.0 / ksize\n",
        "    return cv2.filter2D(frame, -1, kernel)\n",
        "\n",
        "def augment_frame(frame, apply_prob=0.6):\n",
        "    if random.random() > apply_prob:\n",
        "        return frame\n",
        "\n",
        "    # Apply 2–4 random lightweight augmentations\n",
        "    ops = []\n",
        "    if random.random() < 0.7: ops.append(\"brightness\")\n",
        "    if random.random() < 0.7: ops.append(\"contrast\")\n",
        "    if random.random() < 0.4: ops.append(\"noise\")\n",
        "    if random.random() < 0.4: ops.append(\"blur\")\n",
        "    if random.random() < 0.6: ops.append(\"jpeg\")\n",
        "\n",
        "    random.shuffle(ops)\n",
        "\n",
        "    f = frame.copy()\n",
        "    for op in ops:\n",
        "        if op == \"brightness\":\n",
        "            f = adjust_brightness(f, random.uniform(0.8, 1.2))\n",
        "        elif op == \"contrast\":\n",
        "            f = adjust_contrast(f, random.uniform(0.8, 1.25))\n",
        "        elif op == \"noise\":\n",
        "            f = gaussian_noise(f, std=random.uniform(3, 12))\n",
        "        elif op == \"blur\":\n",
        "            f = motion_blur(f, ksize=random.choice([3,5]))\n",
        "        elif op == \"jpeg\":\n",
        "            f = jpeg_compress(f, quality=random.randint(40, 90))\n",
        "\n",
        "    return f\n"
      ],
      "metadata": {
        "id": "-HkfQJUlUBfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 — Feature extraction (EfficientNet-B1)\n",
        "from tensorflow.keras.applications import EfficientNetB1\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "IMG_H = 224; IMG_W = 224\n",
        "BACKBONE = EfficientNetB1(weights=\"imagenet\", include_top=False, pooling='avg', input_shape=(IMG_H, IMG_W, 3))\n",
        "BACKBONE.trainable = False\n",
        "print(\"Backbone ready:\", BACKBONE.name)\n",
        "\n",
        "def extract_features_for_video(video_path, out_feat_path, target_fps=16, augment=False, overwrite=False):\n",
        "    if os.path.exists(out_feat_path) and not overwrite:\n",
        "        print(\"Skipping (exists):\", out_feat_path)\n",
        "        return\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(\"Can't open video: \" + video_path)\n",
        "    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    step = max(1, int(round(orig_fps / target_fps)))\n",
        "    frames = []\n",
        "    idx = 0\n",
        "    while True:\n",
        "        ret, f = cap.read()\n",
        "        if not ret: break\n",
        "        if idx % step == 0:\n",
        "            f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
        "            f = cv2.resize(f, (IMG_W, IMG_H))\n",
        "            if augment:\n",
        "                f = augment_frame(f)\n",
        "            frames.append(f)\n",
        "        idx += 1\n",
        "    cap.release()\n",
        "    if len(frames) == 0:\n",
        "        np.save(out_feat_path, np.zeros((0, BACKBONE.output_shape[-1]), dtype=np.float32))\n",
        "        return\n",
        "    batch = np.array(frames, dtype=np.float32)\n",
        "    batch = preprocess_input(batch)\n",
        "    # run in batches if many frames\n",
        "    feat_list = []\n",
        "    B = 64\n",
        "    for i in range(0, len(batch), B):\n",
        "        sub = batch[i:i+B]\n",
        "        feats = BACKBONE.predict(sub, verbose=0)\n",
        "        feat_list.append(feats)\n",
        "    feats_all = np.concatenate(feat_list, axis=0)\n",
        "    np.save(out_feat_path, feats_all.astype(np.float32))\n",
        "    print(f\"Saved features {out_feat_path} shape={feats_all.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv6K0IS_UD-1",
        "outputId": "967720bb-c7a5-4fc8-adb2-54da025781d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
            "\u001b[1m27018416/27018416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Backbone ready: efficientnetb1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 — extract features for every video in data/raw\n",
        "video_files = sorted([p for p in glob.glob(os.path.join(DATA_RAW, \"*\")) if p.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))])\n",
        "print(\"Found\", len(video_files), \"videos in\", DATA_RAW)\n",
        "\n",
        "for v in tqdm(video_files):\n",
        "    base = Path(v).stem\n",
        "    out = os.path.join(FEATURE_DIR, base + \"-feat.npy\")\n",
        "    extract_features_for_video(v, out, target_fps=16, augment=False, overwrite=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1Z5Fj8RUGoB",
        "outputId": "2ed550d4-c1cc-4be6-9dce-6ef3f4f95176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14 videos in /content/drive/MyDrive/AccidentProject/data/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 1/14 [02:05<27:12, 125.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/1XIS9UOwC_Y-feat.npy shape=(3050, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 2/14 [02:59<16:38, 83.23s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/3KSHg1inZS8-feat.npy shape=(2318, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██▏       | 3/14 [04:35<16:23, 89.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/3LsNpm5y-VA-feat.npy shape=(12132, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 4/14 [05:59<14:31, 87.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/AuQz_-J2kzc-feat.npy shape=(8532, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 5/14 [07:46<14:10, 94.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/Copy of BpMHNEa79lo-feat.npy shape=(10542, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 6/14 [09:28<12:54, 96.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/Copy of L334aqEJxys-feat.npy shape=(9162, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 7/14 [11:30<12:15, 105.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/Copy of N6zJ2inqJn0-feat.npy shape=(9027, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 8/14 [12:32<09:07, 91.31s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/Copy of YzocCFJIbCc-feat.npy shape=(4479, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 9/14 [15:11<09:22, 112.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/Copy of bCr5Grrfw4I-feat.npy shape=(10315, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████▏  | 10/14 [17:11<07:38, 114.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/Copy of hZm7dC8qBO0-feat.npy shape=(7368, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▊  | 11/14 [19:42<06:17, 125.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/Copy of mhQIf0yVU3g-feat.npy shape=(10064, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 12/14 [19:57<03:04, 92.28s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/Copy of xKT7khciy-c-feat.npy shape=(4068, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 13/14 [20:54<01:21, 81.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/HueSPfF0dSQ-feat.npy shape=(5994, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [22:20<00:00, 95.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features /content/drive/MyDrive/AccidentProject/data/features/bSZkOI7eF8k-feat.npy shape=(6534, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 — Build window index files (per-video index list)\n",
        "SEQ_LEN = 48\n",
        "STRIDE = 8  # you can tune stride; smaller -> more samples\n",
        "\n",
        "index_entries = []  # list of (feat_path, start_frame_idx, label)\n",
        "\n",
        "for feat_path in tqdm(sorted(glob.glob(os.path.join(FEATURE_DIR, \"*-feat.npy\")))):\n",
        "    base = Path(feat_path).stem.replace(\"-feat\", \"\")\n",
        "    # find corresponding txt in data/raw\n",
        "    txt_path = None\n",
        "    for ext in ('.txt', '.label'):\n",
        "        cand = os.path.join(DATA_RAW, base + ext)\n",
        "        if os.path.exists(cand): txt_path = cand; break\n",
        "    intervals = load_intervals(txt_path) if txt_path else []\n",
        "    feats = np.load(feat_path, mmap_mode='r')\n",
        "    n_frames = feats.shape[0]\n",
        "    if n_frames < SEQ_LEN:\n",
        "        continue\n",
        "    i = 0\n",
        "    while i + SEQ_LEN <= n_frames:\n",
        "        s_t = i / 16.0\n",
        "        e_t = (i + SEQ_LEN - 1) / 16.0\n",
        "        label = 1 if window_overlaps(s_t, e_t, intervals) else 0\n",
        "        index_entries.append((feat_path, i, label))\n",
        "        i += STRIDE\n",
        "\n",
        "# Save index to disk (so you can reload fast)\n",
        "index_file = os.path.join(PROJECT_DIR, \"dataset_index.npy\")\n",
        "np.save(index_file, np.array(index_entries, dtype=object))\n",
        "print(\"Total windows:\", len(index_entries))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX6nmcyQUIcY",
        "outputId": "7f415e55-b5cf-4bcf-850f-58e8735d00d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:11<00:00,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total windows: 12872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 — feature-level dataset generator (yields batches of shape [B, SEQ_LEN, feat_dim])\n",
        "from sklearn.utils import resample\n",
        "def feature_window_generator(index_file, batch_size=32, shuffle=True, balance=True):\n",
        "    entries = list(np.load(index_file, allow_pickle=True))\n",
        "    idxs = list(range(len(entries)))\n",
        "    while True:\n",
        "        if shuffle: random.shuffle(idxs)\n",
        "        Xb, yb = [], []\n",
        "        for j in idxs:\n",
        "            feat_path, start_idx, label = entries[j]\n",
        "            feats = np.load(feat_path, mmap_mode='r')  # memory efficient\n",
        "            clip = feats[start_idx:start_idx+SEQ_LEN]  # shape (SEQ_LEN, feat_dim)\n",
        "            if clip.shape[0] != SEQ_LEN:\n",
        "                continue\n",
        "            Xb.append(clip)\n",
        "            yb.append(label)\n",
        "            if len(Xb) >= batch_size:\n",
        "                X = np.array(Xb, dtype=np.float32)\n",
        "                y = np.array(yb, dtype=np.float32)\n",
        "                if balance:\n",
        "                    # simple on-batch balancing: oversample positives if needed\n",
        "                    pos_idx = np.where(y==1)[0]\n",
        "                    neg_idx = np.where(y==0)[0]\n",
        "                    if len(pos_idx)>0 and len(pos_idx) < len(neg_idx):\n",
        "                        extra = np.random.choice(pos_idx, size=(len(neg_idx)-len(pos_idx)), replace=True)\n",
        "                        X = np.concatenate([X, X[extra]], axis=0)\n",
        "                        y = np.concatenate([y, y[extra]], axis=0)\n",
        "                perm = np.random.permutation(len(X))\n",
        "                yield X[perm], y[perm]\n",
        "                Xb, yb = [], []\n",
        "        # flush remaining\n",
        "        if len(Xb) > 0:\n",
        "            X = np.array(Xb, dtype=np.float32)\n",
        "            y = np.array(yb, dtype=np.float32)\n",
        "            if balance:\n",
        "                pos_idx = np.where(y==1)[0]\n",
        "                neg_idx = np.where(y==0)[0]\n",
        "                if len(pos_idx)>0 and len(pos_idx) < len(neg_idx):\n",
        "                    extra = np.random.choice(pos_idx, size=(len(neg_idx)-len(pos_idx)), replace=True)\n",
        "                    X = np.concatenate([X, X[extra]], axis=0)\n",
        "                    y = np.concatenate([y, y[extra]], axis=0)\n",
        "            perm = np.random.permutation(len(X))\n",
        "            yield X[perm], y[perm]\n"
      ],
      "metadata": {
        "id": "IZQ_rLSrUJBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 — Transformer encoder on top of features\n",
        "import math\n",
        "def build_transformer_temporal(seq_len=SEQ_LEN, feat_dim=1280, d_model=512, n_layers=3, n_heads=8, ffn_dim=2048, dropout=0.1):\n",
        "    inp = layers.Input(shape=(seq_len, feat_dim), name=\"feat_input\")\n",
        "    # Project features to d_model\n",
        "    x = layers.Dense(d_model, activation=None, name=\"proj\")(inp)\n",
        "    x = layers.LayerNormalization(name=\"ln_in\")(x)\n",
        "    # Positional embeddings (learned)\n",
        "    pos = layers.Embedding(input_dim=seq_len, output_dim=d_model)(tf.range(start=0, limit=seq_len, delta=1))\n",
        "    x = x + pos\n",
        "    # Transformer stacks (Pre-LN pattern)\n",
        "    for i in range(n_layers):\n",
        "        # Multi-head self-attention\n",
        "        x1 = layers.LayerNormalization(name=f\"ln_att_{i}\")(x)\n",
        "        att = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model//n_heads, dropout=dropout, name=f\"mha_{i}\")(x1, x1)\n",
        "        x = layers.Add()([x, att])\n",
        "        # FFN\n",
        "        x2 = layers.LayerNormalization(name=f\"ln_ffn_{i}\")(x)\n",
        "        f = layers.Dense(ffn_dim, activation='relu')(x2)\n",
        "        f = layers.Dropout(dropout)(f)\n",
        "        f = layers.Dense(d_model)(f)\n",
        "        x = layers.Add()([x, f])\n",
        "    # Pooling & head\n",
        "    x = layers.LayerNormalization(name=\"ln_out\")(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    # final output float32 to avoid FP16 loss issues\n",
        "    out = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
        "    model = keras.Model(inputs=inp, outputs=out, name=\"TemporalTransformer\")\n",
        "    return model\n",
        "\n",
        "# quick build\n",
        "feat_sample = np.load(sorted(glob.glob(os.path.join(FEATURE_DIR, \"*-feat.npy\")))[0])\n",
        "feat_dim = feat_sample.shape[1]\n",
        "model = build_transformer_temporal(seq_len=SEQ_LEN, feat_dim=feat_dim, d_model=512, n_layers=3, n_heads=8, ffn_dim=2048)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EVsrozbXUK26",
        "outputId": "6170c425-3f19-4dd2-d46a-d5dca0004098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"TemporalTransformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TemporalTransformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ feat_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1280\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ proj (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m655,872\u001b[0m │ feat_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_in               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ proj[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ ln_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_0            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_0               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_att_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ ln_att_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                     │                   │            │ mha_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_0            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_ffn_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,049,088\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_att_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ ln_att_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ mha_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_ffn_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,049,088\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_att_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ ln_att_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ mha_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_ffn_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,049,088\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_out              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ ln_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m65,664\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ feat_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ proj (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │ feat_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_in               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ proj[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ln_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_0            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_0               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_att_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ ln_att_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                     │                   │            │ mha_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_0            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_ffn_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_att_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ ln_att_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ mha_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_ffn_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_att_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ ln_att_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ mha_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_ffn_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_out              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ln_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,180,865\u001b[0m (38.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,180,865</span> (38.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,180,865\u001b[0m (38.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,180,865</span> (38.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 — training utilities & compile (NO TFA NEEDED)\n",
        "\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "LR = 1e-4\n",
        "opt = AdamW(learning_rate=LR, weight_decay=1e-4)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "\n",
        "ckpt_path = os.path.join(CHECKPOINT_DIR, \"transformer_best.h5\")\n",
        "\n",
        "cb_list = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        ckpt_path,\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_auc',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        patience=6,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n"
      ],
      "metadata": {
        "id": "XRDKD-ApUMWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 — Stage 1 training (transformer head only)\n",
        "BATCH_SIZE = 32\n",
        "STEPS_PER_EPOCH = max(10,  len(np.load(index_file, allow_pickle=True)) // BATCH_SIZE )\n",
        "VAL_SPLIT = 0.1\n",
        "\n",
        "# create generator\n",
        "gen = feature_window_generator(index_file, batch_size=BATCH_SIZE, shuffle=True, balance=True)\n",
        "\n",
        "# If you have a separate val index, use it. Here we do a quick random val split by entries.\n",
        "entries = list(np.load(index_file, allow_pickle=True))\n",
        "random.shuffle(entries)\n",
        "n_val = int(len(entries)*VAL_SPLIT)\n",
        "val_entries = entries[:n_val]\n",
        "train_entries = entries[n_val:]\n",
        "tmp_idx_file = os.path.join(PROJECT_DIR, \"tmp_train_idx.npy\")\n",
        "tmp_val_idx_file = os.path.join(PROJECT_DIR, \"tmp_val_idx.npy\")\n",
        "np.save(tmp_idx_file, np.array(train_entries, dtype=object))\n",
        "np.save(tmp_val_idx_file, np.array(val_entries, dtype=object))\n",
        "\n",
        "train_gen = feature_window_generator(tmp_idx_file, batch_size=BATCH_SIZE, shuffle=True, balance=True)\n",
        "val_gen = feature_window_generator(tmp_val_idx_file, batch_size=BATCH_SIZE, shuffle=False, balance=False)\n",
        "\n",
        "EPOCHS_STAGE1 = 30\n",
        "history = model.fit(train_gen,\n",
        "                    steps_per_epoch=max(10, len(train_entries)//BATCH_SIZE),\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=max(1, len(val_entries)//BATCH_SIZE),\n",
        "                    epochs=EPOCHS_STAGE1,\n",
        "                    callbacks=cb_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyjJ2agmUN5G",
        "outputId": "9d3ca5ea-c37e-4360-a8dc-58cf443cd003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698ms/step - accuracy: 0.6598 - auc: 0.7186 - loss: 0.6264\n",
            "Epoch 1: val_auc improved from -inf to 0.84382, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 757ms/step - accuracy: 0.6599 - auc: 0.7187 - loss: 0.6262 - val_accuracy: 0.7148 - val_auc: 0.8438 - val_loss: 0.5311 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.7865 - auc: 0.8609 - loss: 0.4475\n",
            "Epoch 2: val_auc improved from 0.84382 to 0.90049, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 260ms/step - accuracy: 0.7865 - auc: 0.8609 - loss: 0.4474 - val_accuracy: 0.8016 - val_auc: 0.9005 - val_loss: 0.4049 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8475 - auc: 0.9134 - loss: 0.3508\n",
            "Epoch 3: val_auc improved from 0.90049 to 0.92209, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 264ms/step - accuracy: 0.8475 - auc: 0.9134 - loss: 0.3508 - val_accuracy: 0.7711 - val_auc: 0.9221 - val_loss: 0.4333 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8747 - auc: 0.9386 - loss: 0.2910\n",
            "Epoch 4: val_auc improved from 0.92209 to 0.92837, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 269ms/step - accuracy: 0.8746 - auc: 0.9386 - loss: 0.2910 - val_accuracy: 0.8486 - val_auc: 0.9284 - val_loss: 0.3440 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9044 - auc: 0.9597 - loss: 0.2347\n",
            "Epoch 5: val_auc improved from 0.92837 to 0.95782, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 260ms/step - accuracy: 0.9044 - auc: 0.9597 - loss: 0.2347 - val_accuracy: 0.8625 - val_auc: 0.9578 - val_loss: 0.2956 - learning_rate: 5.0000e-05\n",
            "Epoch 6/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.9177 - auc: 0.9680 - loss: 0.2059\n",
            "Epoch 6: val_auc did not improve from 0.95782\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 253ms/step - accuracy: 0.9177 - auc: 0.9680 - loss: 0.2059 - val_accuracy: 0.8534 - val_auc: 0.9557 - val_loss: 0.3000 - learning_rate: 5.0000e-05\n",
            "Epoch 7/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9205 - auc: 0.9715 - loss: 0.1946\n",
            "Epoch 7: val_auc improved from 0.95782 to 0.96221, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 254ms/step - accuracy: 0.9205 - auc: 0.9715 - loss: 0.1946 - val_accuracy: 0.8884 - val_auc: 0.9622 - val_loss: 0.2437 - learning_rate: 5.0000e-05\n",
            "Epoch 8/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9393 - auc: 0.9814 - loss: 0.1578\n",
            "Epoch 8: val_auc improved from 0.96221 to 0.96702, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 263ms/step - accuracy: 0.9393 - auc: 0.9814 - loss: 0.1578 - val_accuracy: 0.9076 - val_auc: 0.9670 - val_loss: 0.2303 - learning_rate: 2.5000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9509 - auc: 0.9872 - loss: 0.1283\n",
            "Epoch 9: val_auc improved from 0.96702 to 0.97109, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 260ms/step - accuracy: 0.9509 - auc: 0.9872 - loss: 0.1284 - val_accuracy: 0.9052 - val_auc: 0.9711 - val_loss: 0.2187 - learning_rate: 2.5000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9529 - auc: 0.9898 - loss: 0.1173\n",
            "Epoch 10: val_auc did not improve from 0.97109\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 268ms/step - accuracy: 0.9529 - auc: 0.9898 - loss: 0.1173 - val_accuracy: 0.8932 - val_auc: 0.9699 - val_loss: 0.2693 - learning_rate: 2.5000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9567 - auc: 0.9910 - loss: 0.1111\n",
            "Epoch 11: val_auc improved from 0.97109 to 0.97281, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 251ms/step - accuracy: 0.9567 - auc: 0.9910 - loss: 0.1111 - val_accuracy: 0.9131 - val_auc: 0.9728 - val_loss: 0.2311 - learning_rate: 1.2500e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.9628 - auc: 0.9929 - loss: 0.0955\n",
            "Epoch 12: val_auc did not improve from 0.97281\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 250ms/step - accuracy: 0.9628 - auc: 0.9929 - loss: 0.0955 - val_accuracy: 0.9171 - val_auc: 0.9709 - val_loss: 0.2294 - learning_rate: 1.2500e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9634 - auc: 0.9937 - loss: 0.0910\n",
            "Epoch 13: val_auc did not improve from 0.97281\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 244ms/step - accuracy: 0.9634 - auc: 0.9937 - loss: 0.0910 - val_accuracy: 0.9155 - val_auc: 0.9725 - val_loss: 0.2380 - learning_rate: 1.2500e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9669 - auc: 0.9946 - loss: 0.0831\n",
            "Epoch 14: val_auc did not improve from 0.97281\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 243ms/step - accuracy: 0.9669 - auc: 0.9946 - loss: 0.0831 - val_accuracy: 0.9187 - val_auc: 0.9723 - val_loss: 0.2356 - learning_rate: 6.2500e-06\n",
            "Epoch 15/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9736 - auc: 0.9963 - loss: 0.0696\n",
            "Epoch 15: val_auc did not improve from 0.97281\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 244ms/step - accuracy: 0.9736 - auc: 0.9963 - loss: 0.0696 - val_accuracy: 0.9147 - val_auc: 0.9695 - val_loss: 0.2499 - learning_rate: 6.2500e-06\n",
            "Epoch 16/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9766 - auc: 0.9968 - loss: 0.0647\n",
            "Epoch 16: val_auc did not improve from 0.97281\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 247ms/step - accuracy: 0.9766 - auc: 0.9968 - loss: 0.0647 - val_accuracy: 0.9171 - val_auc: 0.9683 - val_loss: 0.2619 - learning_rate: 6.2500e-06\n",
            "Epoch 17/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9791 - auc: 0.9969 - loss: 0.0607\n",
            "Epoch 17: val_auc did not improve from 0.97281\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 248ms/step - accuracy: 0.9791 - auc: 0.9969 - loss: 0.0607 - val_accuracy: 0.9203 - val_auc: 0.9705 - val_loss: 0.2523 - learning_rate: 3.1250e-06\n",
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 — Optional: fine-tune some backbone layers\n",
        "# Only do this if Stage1 is good and you have GPU and patience.\n",
        "FINETUNE = True\n",
        "if FINETUNE:\n",
        "    # load best transformer weights\n",
        "    model.load_weights(ckpt_path)\n",
        "    # Rebuild a model that includes backbone if you want end-to-end; or fine-tune backbone features by re-extracting features with trainable backbone (complex).\n",
        "    # Simpler approach: fine-tune EfficientNet on raw frames with a small classifier and small LR,\n",
        "    # or re-extract features using a slightly fine-tuned backbone. This step is dataset-dependent and more complex.\n",
        "    print(\"Stage 2 suggested approach: re-train backbone on frames using smaller LR or fine-tune last layers offline.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyWuK_MSUQhH",
        "outputId": "de97f2a2-5a45-48d7-ef51-c1513ddf802a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 suggested approach: re-train backbone on frames using smaller LR or fine-tune last layers offline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 — evaluation helpers\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
        "\n",
        "def evaluate_model_on_index(model, index_entries, batch_size=32):\n",
        "    y_true = []\n",
        "    y_prob = []\n",
        "    for i in range(0, len(index_entries), batch_size):\n",
        "        batch = index_entries[i:i+batch_size]\n",
        "        X = []\n",
        "        for feat_path, start, lbl in batch:\n",
        "            f = np.load(feat_path, mmap_mode='r')[start:start+SEQ_LEN]\n",
        "            X.append(f)\n",
        "            y_true.append(lbl)\n",
        "        X = np.array(X, dtype=np.float32)\n",
        "        preds = model.predict(X, verbose=0).ravel()\n",
        "        y_prob.extend(preds.tolist())\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)\n",
        "    auc = roc_auc_score(y_true, y_prob) if y_true.sum()>0 else None\n",
        "    ap = average_precision_score(y_true, y_prob) if y_true.sum()>0 else None\n",
        "    print(\"AUC:\", auc, \"AP:\", ap)\n",
        "    return y_true, y_prob\n",
        "\n",
        "# quick evaluation on validation entries if exists\n",
        "if os.path.exists(tmp_val_idx_file):\n",
        "    val_entries = list(np.load(tmp_val_idx_file, allow_pickle=True))\n",
        "    _y, _p = evaluate_model_on_index(model, val_entries[:2000], batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9dJf4H9URKN",
        "outputId": "bf31aefb-ee08-40de-e486-ee740383c0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9769228586024863 AP: 0.9481615652404849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os\n",
        "video_path = \"/content/drive/MyDrive/AccidentProject/data/raw/HueSPfF0dSQ.mp4\"  # replace with the path you used\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "print(\"exists:\", os.path.exists(video_path))\n",
        "print(\"cap.isOpened():\", cap.isOpened())\n",
        "if cap.isOpened():\n",
        "    print(\"frame_count:\", cap.get(cv2.CAP_PROP_FRAME_COUNT), \"FPS:\", cap.get(cv2.CAP_PROP_FPS))\n",
        "cap.release()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqGWNCgFpXPM",
        "outputId": "3e98ef73-81be-443c-d2ff-3e929c3f0239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exists: True\n",
            "cap.isOpened(): True\n",
            "frame_count: 11988.0 FPS: 25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, cv2\n",
        "video_path = \"/content/drive/MyDrive/AccidentProject/data/raw/HueSPfF0dSQ.mp4\"  # same as above\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "t0 = time.time()\n",
        "count = 0\n",
        "max_reads = 200\n",
        "while count < max_reads:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"end of file at frame\", count); break\n",
        "    count += 1\n",
        "t1 = time.time()\n",
        "cap.release()\n",
        "print(\"Read frames:\", count, \"time:\", t1 - t0, \"s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGGIO7qIpiJ1",
        "outputId": "6471436e-f557-4c9d-800b-65bd9dd45829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read frames: 200 time: 0.4979441165924072 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, time\n",
        "# build one dummy preprocessed image like your pipeline produces:\n",
        "img = np.zeros((1, IMG_H, IMG_W, 3), dtype=np.float32)  # replace IMG_* values from notebook\n",
        "t0 = time.time()\n",
        "_ = BACKBONE.predict(img, verbose=0)\n",
        "print(\"BACKBONE.predict on 1 frame took\", time.time()-t0, \"s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_srbMT8pwsr",
        "outputId": "bfeaaeea-0bb5-46f7-eb1f-290147b19d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BACKBONE.predict on 1 frame took 0.31453800201416016 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dummy feature window\n",
        "dummy = np.zeros((1, SEQ_LEN, feat_dim), dtype=np.float32)\n",
        "t0 = time.time()\n",
        "_ = model.predict(dummy, verbose=0)\n",
        "print(\"temporal model predict time:\", time.time()-t0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hg4SN70pz0t",
        "outputId": "920d2b92-b10a-41db-f423-f430a33bf361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temporal model predict time: 0.15963411331176758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, time, cv2, numpy as np\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "def inference_on_video_fast(video_path, model, target_fps=16, seq_len=SEQ_LEN, stride=4, threshold=0.6, log_dir=LOG_DIR, local_copy=True):\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    # 1) Copy to local /tmp for faster IO (optional)\n",
        "    src = video_path\n",
        "    local_path = src\n",
        "    if local_copy:\n",
        "        base = Path(src).stem\n",
        "        local_path = f\"/tmp/{base}_local.mp4\"\n",
        "        try:\n",
        "            if os.path.exists(local_path):\n",
        "                os.remove(local_path)\n",
        "            shutil.copy(src, local_path)\n",
        "            print(\"Copied to local:\", local_path)\n",
        "        except Exception as e:\n",
        "            print(\"Warning: local copy failed:\", e)\n",
        "            local_path = src\n",
        "\n",
        "    cap = cv2.VideoCapture(local_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(\"Cannot open video: \" + str(local_path))\n",
        "    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    step = max(1, int(round(orig_fps / target_fps)))\n",
        "    print(f\"orig_fps={orig_fps:.2f} step={step} target_fps={target_fps}\")\n",
        "\n",
        "    # buffers\n",
        "    feat_buffer = []     # stores feature vectors\n",
        "    ts_buffer = []       # stores timestamps\n",
        "    frame_batch = []     # small batch for backbone prediction to avoid per-frame predict calls\n",
        "    batch_batch_size = 8 # tune: number of frames to batch before calling BACKBONE.predict\n",
        "    results = []\n",
        "\n",
        "    idx = 0\n",
        "    frames_processed = 0\n",
        "    t_start = time.time()\n",
        "    last_print = time.time()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # sample to target fps\n",
        "        if idx % step == 0:\n",
        "            frames_processed += 1\n",
        "            ts = idx / orig_fps\n",
        "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, (IMG_W, IMG_H))\n",
        "            frame_batch.append(img)\n",
        "            ts_batch_append = ts\n",
        "\n",
        "            # when we have a small batch, run backbone.predict once\n",
        "            if len(frame_batch) >= batch_batch_size:\n",
        "                arr = np.array(frame_batch, dtype=np.float32)\n",
        "                arr = preprocess_input(arr)\n",
        "                t0 = time.time()\n",
        "                feats = BACKBONE.predict(arr, verbose=0)\n",
        "                t1 = time.time()\n",
        "                # append feats in order\n",
        "                for f_idx, fvec in enumerate(feats):\n",
        "                    feat_buffer.append(fvec.astype(np.float32))\n",
        "                    ts_buffer.append(ts - (len(feats)-1 - f_idx) * (1.0/target_fps))  # approximate timestamp\n",
        "                    # maintain buffer length\n",
        "                    if len(feat_buffer) > seq_len:\n",
        "                        feat_buffer.pop(0); ts_buffer.pop(0)\n",
        "                # empty batch\n",
        "                frame_batch = []\n",
        "                # print timing occasionally\n",
        "                if time.time() - last_print > 5.0:\n",
        "                    print(f\"[{datetime.now()}] processed frames={frames_processed}, feat_buf={len(feat_buffer)}, backbone_batch_time={t1-t0:.3f}s\")\n",
        "                    last_print = time.time()\n",
        "\n",
        "            # If buffer full, run temporal model at stride intervals\n",
        "            if len(feat_buffer) == seq_len and (frames_processed % stride == 0):\n",
        "                X = np.array([feat_buffer], dtype=np.float32)\n",
        "                t0m = time.time()\n",
        "                prob = float(model.predict(X, verbose=0)[0,0])\n",
        "                t1m = time.time()\n",
        "                center_ts = ts_buffer[len(ts_buffer)//2]\n",
        "                results.append((center_ts, prob))\n",
        "                if prob >= threshold:\n",
        "                    ts_real = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    log_line = f\"{ts_real} | {video_path} | {center_ts:.2f} | {prob:.4f}\\n\"\n",
        "                    with open(os.path.join(log_dir, \"detections.txt\"), \"a\") as wf:\n",
        "                        wf.write(log_line)\n",
        "                if time.time() - last_print > 5.0:\n",
        "                    print(f\"[TMP] model_pred_time={t1m-t0m:.3f}s prob={prob:.3f} at vt={center_ts:.2f}\")\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "    # flush remaining small frame_batch if any\n",
        "    if len(frame_batch) > 0:\n",
        "        arr = np.array(frame_batch, dtype=np.float32)\n",
        "        arr = preprocess_input(arr)\n",
        "        feats = BACKBONE.predict(arr, verbose=0)\n",
        "        for fvec in feats:\n",
        "            feat_buffer.append(fvec.astype(np.float32))\n",
        "            if len(feat_buffer) > seq_len:\n",
        "                feat_buffer.pop(0)\n",
        "    cap.release()\n",
        "    print(\"Done. total frames processed:\", frames_processed, \"elapsed:\", time.time()-t_start)\n",
        "    return results\n",
        "\n",
        "# Example usage (use your actual video path)\n",
        "res = inference_on_video_fast(\"/content/drive/MyDrive/AccidentProject/data/raw/HueSPfF0dSQ.mp4\", model, target_fps=16, seq_len=48, stride=4, threshold=0.7)\n",
        "print(res[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4R_HKUPUS7E",
        "outputId": "dd575dd5-abc5-4004-900a-21bf1bd2ada8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied to local: /tmp/HueSPfF0dSQ_local.mp4\n",
            "orig_fps=25.00 step=2 target_fps=16\n",
            "[TMP] model_pred_time=0.093s prob=0.000 at vt=9.08\n",
            "[2025-11-20 19:05:48.257833] processed frames=144, feat_buf=48, backbone_batch_time=0.128s\n",
            "[2025-11-20 19:05:53.288433] processed frames=288, feat_buf=48, backbone_batch_time=0.080s\n",
            "[2025-11-20 19:05:58.321229] processed frames=424, feat_buf=48, backbone_batch_time=0.082s\n",
            "[2025-11-20 19:06:03.407261] processed frames=544, feat_buf=48, backbone_batch_time=0.093s\n",
            "[TMP] model_pred_time=0.063s prob=0.673 at vt=53.24\n",
            "[2025-11-20 19:06:08.596828] processed frames=696, feat_buf=48, backbone_batch_time=0.094s\n",
            "[TMP] model_pred_time=0.100s prob=0.000 at vt=64.76\n",
            "[TMP] model_pred_time=0.099s prob=0.000 at vt=64.76\n",
            "[2025-11-20 19:06:13.975095] processed frames=840, feat_buf=48, backbone_batch_time=0.115s\n",
            "[TMP] model_pred_time=0.072s prob=0.000 at vt=75.64\n",
            "[2025-11-20 19:06:19.127949] processed frames=976, feat_buf=48, backbone_batch_time=0.085s\n",
            "[2025-11-20 19:06:24.132359] processed frames=1128, feat_buf=48, backbone_batch_time=0.089s\n",
            "[TMP] model_pred_time=0.064s prob=0.000 at vt=97.40\n",
            "[2025-11-20 19:06:29.261773] processed frames=1248, feat_buf=48, backbone_batch_time=0.093s\n",
            "[TMP] model_pred_time=0.061s prob=1.000 at vt=110.20\n",
            "[TMP] model_pred_time=0.107s prob=1.000 at vt=110.20\n",
            "[2025-11-20 19:06:34.563085] processed frames=1408, feat_buf=48, backbone_batch_time=0.084s\n",
            "[TMP] model_pred_time=0.101s prob=0.000 at vt=121.72\n",
            "[TMP] model_pred_time=0.088s prob=0.000 at vt=121.72\n",
            "[2025-11-20 19:06:39.940857] processed frames=1552, feat_buf=48, backbone_batch_time=0.137s\n",
            "[TMP] model_pred_time=0.063s prob=0.996 at vt=133.24\n",
            "[TMP] model_pred_time=0.065s prob=0.996 at vt=133.24\n",
            "[2025-11-20 19:06:45.207273] processed frames=1696, feat_buf=48, backbone_batch_time=0.088s\n",
            "[TMP] model_pred_time=0.064s prob=0.000 at vt=144.76\n",
            "[TMP] model_pred_time=0.061s prob=0.000 at vt=144.76\n",
            "[2025-11-20 19:06:50.437276] processed frames=1840, feat_buf=48, backbone_batch_time=0.085s\n",
            "[2025-11-20 19:06:55.513451] processed frames=1960, feat_buf=48, backbone_batch_time=0.082s\n",
            "[2025-11-20 19:07:00.603124] processed frames=2088, feat_buf=48, backbone_batch_time=0.078s\n",
            "[TMP] model_pred_time=0.104s prob=0.902 at vt=176.12\n",
            "[TMP] model_pred_time=0.090s prob=0.902 at vt=176.12\n",
            "[2025-11-20 19:07:05.889103] processed frames=2232, feat_buf=48, backbone_batch_time=0.113s\n",
            "[TMP] model_pred_time=0.063s prob=0.000 at vt=187.64\n",
            "[2025-11-20 19:07:11.062385] processed frames=2376, feat_buf=48, backbone_batch_time=0.086s\n",
            "[2025-11-20 19:07:16.132002] processed frames=2528, feat_buf=48, backbone_batch_time=0.091s\n",
            "[2025-11-20 19:07:21.227345] processed frames=2648, feat_buf=48, backbone_batch_time=0.091s\n",
            "[2025-11-20 19:07:26.287095] processed frames=2800, feat_buf=48, backbone_batch_time=0.093s\n",
            "[2025-11-20 19:07:31.356364] processed frames=2920, feat_buf=48, backbone_batch_time=0.115s\n",
            "[2025-11-20 19:07:36.368117] processed frames=3056, feat_buf=48, backbone_batch_time=0.085s\n",
            "[2025-11-20 19:07:41.401878] processed frames=3208, feat_buf=48, backbone_batch_time=0.077s\n",
            "[TMP] model_pred_time=0.064s prob=0.999 at vt=263.80\n",
            "[2025-11-20 19:07:46.514442] processed frames=3328, feat_buf=48, backbone_batch_time=0.078s\n",
            "[TMP] model_pred_time=0.066s prob=0.000 at vt=275.96\n",
            "[2025-11-20 19:07:51.664609] processed frames=3480, feat_buf=48, backbone_batch_time=0.084s\n",
            "[TMP] model_pred_time=0.122s prob=0.000 at vt=288.12\n",
            "[2025-11-20 19:07:56.987962] processed frames=3632, feat_buf=48, backbone_batch_time=0.143s\n",
            "[TMP] model_pred_time=0.067s prob=0.000 at vt=299.00\n",
            "[2025-11-20 19:08:02.152832] processed frames=3768, feat_buf=48, backbone_batch_time=0.081s\n",
            "[TMP] model_pred_time=0.063s prob=0.984 at vt=311.80\n",
            "[TMP] model_pred_time=0.066s prob=0.984 at vt=311.80\n",
            "[2025-11-20 19:08:07.423936] processed frames=3928, feat_buf=48, backbone_batch_time=0.084s\n",
            "[TMP] model_pred_time=0.061s prob=0.000 at vt=322.04\n",
            "[TMP] model_pred_time=0.062s prob=0.000 at vt=322.04\n",
            "[2025-11-20 19:08:12.659984] processed frames=4056, feat_buf=48, backbone_batch_time=0.080s\n",
            "[TMP] model_pred_time=0.060s prob=0.815 at vt=334.84\n",
            "[TMP] model_pred_time=0.064s prob=0.815 at vt=334.84\n",
            "[2025-11-20 19:08:17.885669] processed frames=4216, feat_buf=48, backbone_batch_time=0.080s\n",
            "[TMP] model_pred_time=0.133s prob=0.000 at vt=346.36\n",
            "[2025-11-20 19:08:23.089436] processed frames=4360, feat_buf=48, backbone_batch_time=0.139s\n",
            "[TMP] model_pred_time=0.064s prob=0.000 at vt=357.88\n",
            "[2025-11-20 19:08:28.264829] processed frames=4504, feat_buf=48, backbone_batch_time=0.084s\n",
            "[TMP] model_pred_time=0.066s prob=0.988 at vt=370.68\n",
            "[2025-11-20 19:08:33.381807] processed frames=4664, feat_buf=48, backbone_batch_time=0.082s\n",
            "[2025-11-20 19:08:38.415231] processed frames=4784, feat_buf=48, backbone_batch_time=0.084s\n",
            "[TMP] model_pred_time=0.061s prob=0.975 at vt=393.08\n",
            "[2025-11-20 19:08:43.560012] processed frames=4944, feat_buf=48, backbone_batch_time=0.083s\n",
            "[TMP] model_pred_time=0.121s prob=0.001 at vt=404.60\n",
            "[TMP] model_pred_time=0.124s prob=0.001 at vt=404.60\n",
            "[2025-11-20 19:08:48.912051] processed frames=5088, feat_buf=48, backbone_batch_time=0.118s\n",
            "[2025-11-20 19:08:53.944818] processed frames=5216, feat_buf=48, backbone_batch_time=0.079s\n",
            "[TMP] model_pred_time=0.066s prob=0.000 at vt=427.00\n",
            "[2025-11-20 19:08:59.069455] processed frames=5368, feat_buf=48, backbone_batch_time=0.093s\n",
            "[2025-11-20 19:09:04.073579] processed frames=5488, feat_buf=48, backbone_batch_time=0.093s\n",
            "[TMP] model_pred_time=0.059s prob=0.992 at vt=449.40\n",
            "[TMP] model_pred_time=0.060s prob=0.992 at vt=449.40\n",
            "[2025-11-20 19:09:09.325571] processed frames=5648, feat_buf=48, backbone_batch_time=0.089s\n",
            "[2025-11-20 19:09:14.363142] processed frames=5792, feat_buf=48, backbone_batch_time=0.104s\n",
            "[2025-11-20 19:09:19.377274] processed frames=5920, feat_buf=48, backbone_batch_time=0.088s\n",
            "Done. total frames processed: 5994 elapsed: 244.67513036727905\n",
            "[(2.0425, 0.9940873384475708), (2.0425, 0.9940873384475708), (2.6825, 0.9957622289657593), (2.6825, 0.9957622289657593), (3.3225, 0.994296669960022), (3.3225, 0.994296669960022), (3.9625000000000004, 0.9945160746574402), (3.9625000000000004, 0.9945160746574402), (4.6025, 0.992984414100647), (4.6025, 0.992984414100647), (5.2425, 0.994634211063385), (5.2425, 0.994634211063385), (5.8825, 0.9942726492881775), (5.8825, 0.9942726492881775), (6.5225, 0.9782950282096863), (6.5225, 0.9782950282096863), (7.1625, 0.1060991957783699), (7.1625, 0.1060991957783699), (7.8025, 0.0035338958259671926), (7.8025, 0.0035338958259671926)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15 — save/export final transformer head\n",
        "model.save(os.path.join(EXPORT_DIR, \"transformer_temporal_head.keras\"), include_optimizer=False)\n",
        "print(\"Saved transformer to\", os.path.join(EXPORT_DIR, \"transformer_temporal_head.keras\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbY_A62KUUa1",
        "outputId": "7804e415-8084-4bd9-c018-a4bbd070c0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved transformer to /content/drive/MyDrive/AccidentProject/exports/models/transformer_temporal_head.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation cell: window-level + event-level metrics, save CSV + summary\n",
        "import numpy as np, os, pandas as pd, math, itertools\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
        "                             confusion_matrix, classification_report,\n",
        "                             precision_recall_fscore_support, roc_curve,\n",
        "                             precision_recall_curve)\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from datetime import timedelta\n",
        "\n",
        "# --- Params (tweak as needed) ---\n",
        "THRESHOLD = 0.5            # window-level threshold for binary decisions\n",
        "EVENT_TOLERANCE = 3.0      # seconds: predicted event matches GT if overlapping within +/- this many seconds\n",
        "MIN_EVENT_DURATION = 0.5   # seconds: ignore super-short predicted events\n",
        "SAVE_DIR = os.path.join(PROJECT_DIR, \"eval\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# print the uploaded notebook path (developer-provided file)\n",
        "NOTEBOOK_PATH = \"/mnt/data/accident_tsm_full_notebook_reupload.ipynb\"\n",
        "print(\"Notebook file (local):\", NOTEBOOK_PATH)\n",
        "\n",
        "# --- Load validation entries ---\n",
        "if 'tmp_val_idx_file' in globals() and os.path.exists(tmp_val_idx_file):\n",
        "    val_entries = list(np.load(tmp_val_idx_file, allow_pickle=True))\n",
        "else:\n",
        "    entries = list(np.load(index_file, allow_pickle=True))\n",
        "    # fallback: take last 10% as val\n",
        "    n_val = max(1, int(len(entries) * 0.1))\n",
        "    val_entries = entries[:n_val]\n",
        "\n",
        "print(\"Validation windows:\", len(val_entries))\n",
        "\n",
        "# --- Helper to load batch windows and predict in reasonable sized batches ---\n",
        "def predict_on_entries(model, entries, batch_size=64):\n",
        "    y_true = []\n",
        "    y_prob = []\n",
        "    rows = []\n",
        "    feat_dim_local = np.load(entries[0][0], mmap_mode='r').shape[1]\n",
        "    for i in range(0, len(entries), batch_size):\n",
        "        batch = entries[i:i+batch_size]\n",
        "        X = np.zeros((len(batch), SEQ_LEN, feat_dim_local), dtype=np.float32)\n",
        "        for j, (feat_path, start_idx, lbl) in enumerate(batch):\n",
        "            feats = np.load(feat_path, mmap_mode='r')\n",
        "            clip = feats[int(start_idx): int(start_idx) + SEQ_LEN]\n",
        "            if clip.shape[0] != SEQ_LEN:\n",
        "                # pad (shouldn't happen usually)\n",
        "                pad = np.zeros((SEQ_LEN - clip.shape[0], feats.shape[1]), dtype=np.float32)\n",
        "                clip = np.concatenate([clip, pad], axis=0)\n",
        "            X[j] = clip\n",
        "            y_true.append(int(lbl))\n",
        "        preds = model.predict(X, verbose=0).ravel()\n",
        "        y_prob.extend(preds.tolist())\n",
        "        # store rows for CSV: per-window (video basename, start_frame, center_time, true, prob)\n",
        "        for k, (feat_path, start_idx, lbl) in enumerate(batch):\n",
        "            base = os.path.basename(feat_path).replace(\"-feat.npy\", \"\")\n",
        "            center_frame = int(start_idx) + SEQ_LEN // 2\n",
        "            center_time = center_frame / 16.0  # assuming feature fps was 16\n",
        "            rows.append({\n",
        "                \"video\": base,\n",
        "                \"feat_path\": feat_path,\n",
        "                \"start_idx\": int(start_idx),\n",
        "                \"center_time\": center_time,\n",
        "                \"true\": int(lbl),\n",
        "                \"prob\": float(preds[k])\n",
        "            })\n",
        "    return np.array(y_true), np.array(y_prob), pd.DataFrame(rows)\n",
        "\n",
        "# Run predictions on validation\n",
        "y_true, y_prob, df_preds = predict_on_entries(model, val_entries, batch_size=64)\n",
        "print(\"Predictions done. Examples:\\n\", df_preds.head())\n",
        "\n",
        "# --- Window-level metrics ---\n",
        "auc = roc_auc_score(y_true, y_prob) if y_true.sum() > 0 else float('nan')\n",
        "ap = average_precision_score(y_true, y_prob) if y_true.sum() > 0 else float('nan')\n",
        "y_pred = (y_prob >= THRESHOLD).astype(int)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "report = classification_report(y_true, y_pred, zero_division=0)\n",
        "\n",
        "print(\"\\n=== WINDOW-LEVEL METRICS ===\")\n",
        "print(\"Window AUC:\", auc)\n",
        "print(\"Window AP (PR AUC):\", ap)\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"Precision:\", precision, \"Recall:\", recall, \"F1:\", f1)\n",
        "print(\"\\nClassification report:\\n\", report)\n",
        "\n",
        "# Save per-window CSV and metrics\n",
        "csv_path = os.path.join(SAVE_DIR, \"window_level_predictions.csv\")\n",
        "df_preds.to_csv(csv_path, index=False)\n",
        "metrics = {\n",
        "    \"window_auc\": float(auc),\n",
        "    \"window_ap\": float(ap),\n",
        "    \"precision\": float(precision),\n",
        "    \"recall\": float(recall),\n",
        "    \"f1\": float(f1),\n",
        "    \"threshold\": float(THRESHOLD),\n",
        "    \"n_val_windows\": int(len(val_entries))\n",
        "}\n",
        "with open(os.path.join(SAVE_DIR, \"window_metrics.json\"), \"w\") as wf:\n",
        "    json.dump(metrics, wf, indent=2)\n",
        "print(\"Saved CSV ->\", csv_path)\n",
        "print(\"Saved metrics ->\", os.path.join(SAVE_DIR, \"window_metrics.json\"))\n",
        "\n",
        "# --- Plot ROC and PR curves (quick) ---\n",
        "fpr, tpr, _ = roc_curve(y_true, y_prob) if len(np.unique(y_true))>1 else (None, None, None)\n",
        "precisions, recalls, _ = precision_recall_curve(y_true, y_prob)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "if fpr is not None:\n",
        "    plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC\")\n",
        "    plt.legend()\n",
        "else:\n",
        "    plt.text(0.2,0.5,\"Not enough class variety for ROC\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(recalls, precisions, label=f\"AP={ap:.3f}\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# === EVENT-LEVEL METRICS ===\n",
        "# Convert ground-truth windows to per-video event lists (from val_entries -> use underlying txt intervals)\n",
        "def build_video_intervals_from_index(entries):\n",
        "    # returns dict: video_basename -> list of (start_sec, end_sec) ground truth intervals\n",
        "    vid_intervals = {}\n",
        "    for feat_path, start_idx, lbl in entries:\n",
        "        base = os.path.basename(feat_path).replace(\"-feat.npy\", \"\")\n",
        "        # find raw txt path in DATA_RAW with same base (common extensions)\n",
        "        txt_path = None\n",
        "        for ext in ('.txt', '.label'):\n",
        "            p = os.path.join(DATA_RAW, base + ext)\n",
        "            if os.path.exists(p):\n",
        "                txt_path = p; break\n",
        "        if base not in vid_intervals:\n",
        "            vid_intervals[base] = set()\n",
        "        if int(lbl) == 1:\n",
        "            # the window corresponds to a positive; convert window start/end times\n",
        "            s_frame = int(start_idx)\n",
        "            e_frame = int(start_idx) + SEQ_LEN - 1\n",
        "            s_sec = s_frame / 16.0\n",
        "            e_sec = e_frame / 16.0\n",
        "            # store interval with small merge tolerance\n",
        "            vid_intervals[base].add((round(s_sec,3), round(e_sec,3)))\n",
        "    # Convert sets -> merged sorted lists\n",
        "    for k in list(vid_intervals.keys()):\n",
        "        ints = sorted(list(vid_intervals[k]))\n",
        "        merged = []\n",
        "        for a,b in ints:\n",
        "            if not merged or a > merged[-1][1] + 1e-6:\n",
        "                merged.append([a,b])\n",
        "            else:\n",
        "                merged[-1][1] = max(merged[-1][1], b)\n",
        "        vid_intervals[k] = [(x[0], x[1]) for x in merged]\n",
        "    return vid_intervals\n",
        "\n",
        "# Build predicted events per-video by merging contiguous predicted positive windows\n",
        "def build_predicted_events(df_preds, threshold=THRESHOLD, tolerance_merge= (SEQ_LEN/16.0)/2.0):\n",
        "    # tolerance_merge approx half-window so adjacent windows join\n",
        "    pred_events = {}\n",
        "    df_pos = df_preds[df_preds['prob'] >= threshold].copy()\n",
        "    for base, grp in df_pos.groupby('video'):\n",
        "        times = sorted(grp['center_time'].tolist())\n",
        "        # merge times into continuous events if centers are within tolerance_merge seconds\n",
        "        events = []\n",
        "        for t in times:\n",
        "            if not events:\n",
        "                events.append([t - SEQ_LEN/16.0/2.0, t + SEQ_LEN/16.0/2.0])\n",
        "            else:\n",
        "                if t <= events[-1][1] + tolerance_merge:\n",
        "                    # extend\n",
        "                    events[-1][1] = max(events[-1][1], t + SEQ_LEN/16.0/2.0)\n",
        "                else:\n",
        "                    events.append([t - SEQ_LEN/16.0/2.0, t + SEQ_LEN/16.0/2.0])\n",
        "        # filter tiny events\n",
        "        filtered = [(max(0, round(a,3)), round(b,3)) for a,b in events if (b - a) >= MIN_EVENT_DURATION]\n",
        "        pred_events[base] = filtered\n",
        "    return pred_events\n",
        "\n",
        "# Ground-truth: parse .txt intervals for each video (full GT intervals)\n",
        "def load_gt_intervals_for_val(entries):\n",
        "    gt = {}\n",
        "    bases = set([os.path.basename(e[0]).replace(\"-feat.npy\",\"\") for e in entries])\n",
        "    for base in bases:\n",
        "        # find txt file\n",
        "        txt_path = None\n",
        "        for ext in ('.txt', '.label'):\n",
        "            p = os.path.join(DATA_RAW, base + ext)\n",
        "            if os.path.exists(p): txt_path = p; break\n",
        "        intervals = []\n",
        "        if txt_path:\n",
        "            for line in open(txt_path,'r'):\n",
        "                line=line.strip()\n",
        "                if not line: continue\n",
        "                if \"-\" in line:\n",
        "                    a,b = line.split(\"-\")\n",
        "                elif \"to\" in line:\n",
        "                    a,b = line.split(\"to\")\n",
        "                else:\n",
        "                    continue\n",
        "                try:\n",
        "                    intervals.append((to_seconds(a), to_seconds(b)))\n",
        "                except:\n",
        "                    continue\n",
        "        gt[base] = intervals\n",
        "    return gt\n",
        "\n",
        "gt_intervals = load_gt_intervals_for_val(val_entries)\n",
        "pred_events = build_predicted_events(df_preds, threshold=THRESHOLD)\n",
        "\n",
        "# Event-level matching. A predicted event is a true positive if it overlaps any GT interval by > 0 seconds within tolerance\n",
        "def overlap(a_start,a_end,b_start,b_end):\n",
        "    return not (a_end < b_start or b_end < a_start)\n",
        "\n",
        "event_TP = 0; event_FP = 0; event_FN = 0\n",
        "per_video_matches = {}\n",
        "for video, gt_list in gt_intervals.items():\n",
        "    preds = pred_events.get(video, [])\n",
        "    matched_gt = set()\n",
        "    for pe in preds:\n",
        "        p_s,p_e = pe\n",
        "        matched = False\n",
        "        for gi_idx, (g_s,g_e) in enumerate(gt_list):\n",
        "            # consider match if overlap within tolerance or center within +/- EVENT_TOLERANCE\n",
        "            if overlap(p_s, p_e, g_s - EVENT_TOLERANCE, g_e + EVENT_TOLERANCE):\n",
        "                matched = True\n",
        "                matched_gt.add(gi_idx)\n",
        "                break\n",
        "        if matched:\n",
        "            event_TP += 1\n",
        "        else:\n",
        "            event_FP += 1\n",
        "    # ground-truth events that were not matched\n",
        "    event_FN += max(0, len(gt_list) - len(matched_gt))\n",
        "    per_video_matches[video] = {\"gt\": gt_list, \"pred\": preds, \"matched_gt_count\": len(matched_gt)}\n",
        "\n",
        "# compute event-level metrics\n",
        "ev_precision = event_TP / (event_TP + event_FP) if (event_TP + event_FP)>0 else 0.0\n",
        "ev_recall = event_TP / (event_TP + event_FN) if (event_TP + event_FN)>0 else 0.0\n",
        "ev_f1 = 2*ev_precision*ev_recall/(ev_precision+ev_recall) if (ev_precision+ev_recall)>0 else 0.0\n",
        "\n",
        "print(\"\\n=== EVENT-LEVEL METRICS ===\")\n",
        "print(\"Event TP:\", event_TP, \"FP:\", event_FP, \"FN:\", event_FN)\n",
        "print(\"Event Precision:\", round(ev_precision,4), \"Event Recall:\", round(ev_recall,4), \"Event F1:\", round(ev_f1,4))\n",
        "\n",
        "# Save event-level results and pred_events/gt to JSON\n",
        "with open(os.path.join(SAVE_DIR, \"event_results.json\"), \"w\") as wf:\n",
        "    json.dump({\n",
        "        \"event_tp\": int(event_TP),\n",
        "        \"event_fp\": int(event_FP),\n",
        "        \"event_fn\": int(event_FN),\n",
        "        \"event_precision\": ev_precision,\n",
        "        \"event_recall\": ev_recall,\n",
        "        \"event_f1\": ev_f1,\n",
        "        \"pred_events\": pred_events,\n",
        "        \"gt_intervals\": gt_intervals,\n",
        "        \"per_video_matches\": per_video_matches\n",
        "    }, wf, indent=2)\n",
        "\n",
        "print(\"Saved event-level JSON ->\", os.path.join(SAVE_DIR, \"event_results.json\"))\n",
        "\n",
        "# Save a human-readable summary\n",
        "with open(os.path.join(SAVE_DIR, \"eval_summary.txt\"), \"w\") as wf:\n",
        "    wf.write(\"=== WINDOW-LEVEL METRICS ===\\n\")\n",
        "    wf.write(json.dumps(metrics, indent=2) + \"\\n\\n\")\n",
        "    wf.write(\"Confusion matrix:\\n\" + str(cm) + \"\\n\\n\")\n",
        "    wf.write(\"Classification report:\\n\" + report + \"\\n\\n\")\n",
        "    wf.write(\"=== EVENT-LEVEL METRICS ===\\n\")\n",
        "    wf.write(f\"event_tp: {event_TP}\\nevent_fp: {event_FP}\\nevent_fn: {event_FN}\\nevent_precision: {ev_precision}\\nevent_recall: {ev_recall}\\nevent_f1: {ev_f1}\\n\")\n",
        "print(\"Saved human summary ->\", os.path.join(SAVE_DIR, \"eval_summary.txt\"))\n",
        "\n",
        "print(\"\\nDone. CSV and JSON results are in:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e2ti8wBOuwPX",
        "outputId": "b9785fe3-760a-48dd-ebb5-313c2eab284c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook file (local): /mnt/data/accident_tsm_full_notebook_reupload.ipynb\n",
            "Validation windows: 1287\n",
            "Predictions done. Examples:\n",
            "                  video                                          feat_path  \\\n",
            "0  Copy of BpMHNEa79lo  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "1          3KSHg1inZS8  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "2          3LsNpm5y-VA  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "3  Copy of mhQIf0yVU3g  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "4          3LsNpm5y-VA  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "\n",
            "   start_idx  center_time  true      prob  \n",
            "0       7856        492.5     0  0.000655  \n",
            "1       2016        127.5     1  0.999949  \n",
            "2       9968        624.5     1  0.997155  \n",
            "3       5264        330.5     0  0.000003  \n",
            "4       7568        474.5     0  0.000002  \n",
            "\n",
            "=== WINDOW-LEVEL METRICS ===\n",
            "Window AUC: 0.9769228586024863\n",
            "Window AP (PR AUC): 0.9481615652404849\n",
            "Confusion matrix:\n",
            " [[805  87]\n",
            " [ 26 369]]\n",
            "Precision: 0.8092105263157895 Recall: 0.9341772151898734 F1: 0.8672150411280846\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.90      0.93       892\n",
            "           1       0.81      0.93      0.87       395\n",
            "\n",
            "    accuracy                           0.91      1287\n",
            "   macro avg       0.89      0.92      0.90      1287\n",
            "weighted avg       0.92      0.91      0.91      1287\n",
            "\n",
            "Saved CSV -> /content/drive/MyDrive/AccidentProject/eval/window_level_predictions.csv\n",
            "Saved metrics -> /content/drive/MyDrive/AccidentProject/eval/window_metrics.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EVENT-LEVEL METRICS ===\n",
            "Event TP: 276 FP: 4 FN: 125\n",
            "Event Precision: 0.9857 Event Recall: 0.6883 Event F1: 0.8106\n",
            "Saved event-level JSON -> /content/drive/MyDrive/AccidentProject/eval/event_results.json\n",
            "Saved human summary -> /content/drive/MyDrive/AccidentProject/eval/eval_summary.txt\n",
            "\n",
            "Done. CSV and JSON results are in: /content/drive/MyDrive/AccidentProject/eval\n"
          ]
        }
      ]
    }
  ]
}